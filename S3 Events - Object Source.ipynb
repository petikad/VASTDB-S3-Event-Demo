{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd2b220-c10c-476f-9839-d7fb0733d85a",
   "metadata": {},
   "source": [
    "# S3 Object Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedad1f2-8bd6-4fa3-85af-c70726e9d0d9",
   "metadata": {},
   "source": [
    "## Install and Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ef70c-fb3a-4998-9c7e-c2fff8cb41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3==1.35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c877749-5c4e-4a34-a62e-ccc2d5adbf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import time\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import logging\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee19f7-3fe6-417b-a68b-2bd67e228fca",
   "metadata": {},
   "source": [
    "## Load Demo State and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cdbd69-65d0-4988-9099-ee5c32d4ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spinner = ['/', '-', '\\\\', '|']\n",
    "\n",
    "# Read in the Demo Variables and values.\n",
    "with open(\"demo_state.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "# Dynamically create python variables needed for the Demo.\n",
    "for key, value in data.items():\n",
    "    globals()[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fffbc-c1ae-47ea-866a-0651398ec93d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ebc665-e8fc-43a5-8d63-d9dfbb0b308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_tags():\n",
    "    \"\"\"\n",
    "      Used to generate data for the demo.\n",
    "    \"\"\"\n",
    "    Projects = [\n",
    "        \"PB\",      # Permian Basin (Texas and New Mexico)\n",
    "        \"EFS\",     # Eagle Ford Shale (Texas)\n",
    "        \"BF\",      # Bakken Formation (North Dakota)\n",
    "        \"Prudhoe\", # Prudhoe Bay (Alaska)\n",
    "        \"GoM\"      # Gulf of Mexico (offshore)\n",
    "    ]\n",
    "    Hydrostatic_pressures = [\n",
    "        \"2340\", \n",
    "        \"5200\", \n",
    "        \"9360\", \n",
    "        \"14560\"\n",
    "    ]\n",
    "    Asset_Types = [\n",
    "        \"EXP\",   # Exploration Wells: Drilled to discover new resources\n",
    "        \"DEV\",   # Development Wells: Drilled to extract known reserves\n",
    "        \"PROD\",  # Production Wells: Actively producing oil or gas\n",
    "        \"INJ\"    # Injection Wells: Used for enhanced recovery (e.g., water or COâ‚‚ injection)\n",
    "    ]\n",
    "    Project = random.choice(Projects)\n",
    "    BPD = str(random.randint(1000,3000))\n",
    "    PSI = str(random.randint(2000,5000))\n",
    "    Temperature = str(random.randint(140,248))\n",
    "    HSP = random.choice(Hydrostatic_pressures)\n",
    "    Asset_type = random.choice(Asset_Types)\n",
    "    \n",
    "    return BPD, PSI, Temperature, Project, Asset_type, HSP\n",
    "\n",
    "def generate_random_objectname(length=8,prefix=None):\n",
    "    \"\"\"\n",
    "        Used to generate an Object name for the demo.\n",
    "    \"\"\"\n",
    "    file_extensions = [\n",
    "    \".txt\",    # Plain text file\n",
    "    \".csv\",    # Comma-separated values\n",
    "    \".json\",   # JSON data file\n",
    "    \".xml\",    # XML file\n",
    "    \".pdf\",    # PDF document\n",
    "    \".docx\",   # Microsoft Word document\n",
    "    \".xlsx\",   # Microsoft Excel spreadsheet\n",
    "    \".jpg\",    # JPEG image\n",
    "    \".png\",    # PNG image\n",
    "    \".mp4\",    # MPEG-4 video\n",
    "    \".zip\",    # ZIP archive\n",
    "    ]\n",
    "    if prefix: \n",
    "        prefix = prefix + '/'\n",
    "    basename = ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "    extension = random.choice(file_extensions)\n",
    "    return prefix + basename + extension\n",
    "\n",
    "def upload_file_to_s3(file_name, bucket_name, object_name, tags, endpoint_url):\n",
    "    \"\"\"\n",
    "       Upload an object via S3v4 to VAST Cluster and update User Tags.\n",
    "    \"\"\"\n",
    "    # Initialize the S3 client with custom endpoint\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=endpoint_url,\n",
    "        aws_access_key_id=S3_ACCESS_KEY,\n",
    "        aws_secret_access_key=S3_SECRET_KEY,\n",
    "        config=Config(signature_version='s3v4', \n",
    "                      parameter_validation=False, \n",
    "                      s3={'payload_signing_enabled':False,'addressing_style':'path','checksum_algorithm': None}\n",
    "                     ),\n",
    "        verify=False  # Set to False if the endpoint doesn't use SSL (http)\n",
    "    )\n",
    "\n",
    "    # Upload file to S3\n",
    "    try:\n",
    "        # Upload the file\n",
    "        s3_client.upload_file(file_name, bucket_name, object_name)\n",
    "        #time.sleep(5)\n",
    "        # Add tags to the uploaded object\n",
    "        s3_client.put_object_tagging(\n",
    "            Bucket=bucket_name,\n",
    "            Key=object_name,\n",
    "            Tagging={\n",
    "                'TagSet': tags\n",
    "            }\n",
    "        )\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error uploading {file_name} to {bucket_name}: {e}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a05dd9-b7eb-4399-9e18-b62a468d0578",
   "metadata": {},
   "source": [
    "## Simulate Object Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c7b1f-f36b-4e98-ae1f-3a771f68f5ee",
   "metadata": {},
   "source": [
    "### Put Objects and Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a86a4-b6a2-4f34-abee-eddd5189e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_number = 500\n",
    "file_name = 'demo_state.json'\n",
    "spin = 0 \n",
    "last_percent = -1\n",
    "start_time = time.time()\n",
    "s3_endpoint_url = f\"http://{vip_pool_ip}\"\n",
    "s3_objects = []\n",
    "for i in range(create_number):\n",
    "    \n",
    "    gen_tags = generate_random_tags()\n",
    "    tags = [\n",
    "        { 'Key': 'BPD', 'Value': gen_tags[0]},\n",
    "        { 'Key': 'PSI', 'Value': gen_tags[1]},\n",
    "        { 'Key': 'Temperature', 'Value': gen_tags[2]},\n",
    "        { 'Key': 'Project', 'Value': gen_tags[3]},\n",
    "        { 'Key': 'Asset_type', 'Value': gen_tags[4]},\n",
    "        { 'Key': 'HSP', 'Value': gen_tags[5]}\n",
    "    ]\n",
    "    s3_object_key = generate_random_objectname(length=8,prefix='site1')\n",
    "    s3_objects.append(s3_object_key)\n",
    "    upload_file_to_s3(file_name, S3_bucket_name, s3_object_key , tags, s3_endpoint_url)\n",
    "    \n",
    "    # Provide feedback to user on progress for every 10% completed.\n",
    "    percent = (i * 100) // create_number\n",
    "    if percent % 10 == 0 and percent != last_percent:\n",
    "      spin += 1\n",
    "      if spin > len(spinner) - 1:\n",
    "        spin = 0\n",
    "      last_percent = percent\n",
    "    sys.stdout.write(f'\\r{spinner[spin]}')  \n",
    "    sys.stdout.flush()       \n",
    "print(f\"Elapsed time: {time.time() - start_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152ab55-72c9-4697-94f5-487beb8b149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e14f3-5ad4-4870-a18c-5077491eb4e4",
   "metadata": {},
   "source": [
    "#### Query Kafka now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d611b-3b47-4a5a-b545-c0ef07ded810",
   "metadata": {},
   "source": [
    "* Switch to the S3 Events - Kafka Notebook and process the events that were generated.\n",
    "* Using the VAST DB Notebook run SQL queries against the events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3515f0-94b1-459a-8511-5745c26adae4",
   "metadata": {},
   "source": [
    "### Delete 10% of the Objects from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cda46-9f7d-4236-be2d-66a0a8560d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Randomly select 10% of the Object that were created and delete them.\n",
    "#\n",
    "num_to_delete = max(1, len(s3_objects) // 10)  # At least 1\n",
    "objects_to_delete = random.sample(s3_objects, num_to_delete)\n",
    "objects_deleted = 0 \n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=s3_endpoint_url,\n",
    "    aws_access_key_id=S3_ACCESS_KEY,\n",
    "    aws_secret_access_key=S3_SECRET_KEY,\n",
    "    config=Config(\n",
    "        signature_version='s3v4',\n",
    "        parameter_validation=False,\n",
    "        s3={\n",
    "            'payload_signing_enabled': False, 'addressing_style': 'path', 'checksum_algorithm': None\n",
    "        }\n",
    "    ),\n",
    "    verify=False\n",
    ")\n",
    "\n",
    "for key in objects_to_delete:\n",
    "    try:\n",
    "        s3_client.delete_object(Bucket= S3_bucket_name, Key=key)\n",
    "        objects_deleted += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {key}: {e}\")\n",
    "print(f\"{objects_deleted} objects were deleted from the {S3_bucket_name} bucket.\")\n",
    "print(f\"Here are 10 Objects that were marked for deletion:\")\n",
    "random.sample(objects_to_delete, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5faf57-2aba-4cd2-a25a-cc62eab69442",
   "metadata": {},
   "source": [
    "#### Query Kafka now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d020449-2f94-4550-b98d-5ea23e9577fb",
   "metadata": {},
   "source": [
    "* Switch to the Kafka Notebook and process the events that were generated.\n",
    "* Using the VAST DB Notebook run SQL queries against the events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80786a9f-f1d3-4359-9aed-d7e79abe0b95",
   "metadata": {},
   "source": [
    "### Add User Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7674b0-239f-4525-b3b5-9d71800d8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_objects_with_extension(bucket_name, s3_client, extension=\".txt\", new_tags=None):\n",
    "    \"\"\"\n",
    "       Adds new tags to list of Objects that have a given extension.\n",
    "    \"\"\"\n",
    "    count = 0 \n",
    "    if new_tags is None:\n",
    "        new_tags = {}\n",
    "\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name)\n",
    "\n",
    "    for page in page_iterator:\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if not key.endswith(extension):\n",
    "                continue\n",
    "\n",
    "            # Get existing tags\n",
    "            try:\n",
    "                response = s3_client.get_object_tagging(Bucket=bucket_name, Key=key)\n",
    "                tag_set = response.get('TagSet', [])\n",
    "            except s3_client.exceptions.NoSuchTagSet:\n",
    "                tag_set = []\n",
    "\n",
    "            # Merge existing tags with new tags\n",
    "            tag_dict = {tag['Key']: tag['Value'] for tag in tag_set}\n",
    "            tag_dict.update(new_tags)\n",
    "\n",
    "            # Format tag set\n",
    "            updated_tag_set = [{'Key': k, 'Value': v} for k, v in tag_dict.items()]\n",
    "\n",
    "            # Apply updated tags\n",
    "            s3_client.put_object_tagging(\n",
    "                Bucket=bucket_name,\n",
    "                Key=key,\n",
    "                Tagging={'TagSet': updated_tag_set}\n",
    "            )\n",
    "            count += 1\n",
    "    return count         \n",
    "\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=s3_endpoint_url,\n",
    "    aws_access_key_id=S3_ACCESS_KEY,\n",
    "    aws_secret_access_key=S3_SECRET_KEY,\n",
    "    config=Config(\n",
    "        signature_version='s3v4',\n",
    "        parameter_validation=False,\n",
    "        s3={\n",
    "            'payload_signing_enabled': False, 'addressing_style': 'path', 'checksum_algorithm': None\n",
    "        }\n",
    "    ),\n",
    "    verify=False\n",
    ")\n",
    "update_extension = \".docx\"\n",
    "proc_count = tag_objects_with_extension(S3_bucket_name, s3_client, extension=update_extension, new_tags={\"processed\": \"False\", \"ASCII\": \"False\"})\n",
    "print(f\"{proc_count} `{update_extension}` Objects were updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cbe72-eb49-4433-a44a-ae563bf0126f",
   "metadata": {},
   "source": [
    "#### Query Kafka now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c6ac8-ce9b-4732-a522-dcaf5368f94d",
   "metadata": {},
   "source": [
    "* Switch to the Kafka Notebook and process the events that were generated.\n",
    "* Using the VAST DB Notebook run SQL queries against the events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4ab78-a34d-4666-9231-8bc94b435126",
   "metadata": {},
   "source": [
    "### Update User Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c9d74-d3a6-4519-a5cc-7136c75d20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_s3_object_tag_value(s3_client, bucket_name, tag_key, old_value, new_value):\n",
    "    \"\"\"\n",
    "    Iterates over all objects in the specified S3 bucket and updates the value of a specific tag\n",
    "    if it matches the provided old value.\n",
    "\n",
    "    :param s3_client: An initialized boto3 S3 client\n",
    "    :param bucket_name: Name of the S3 bucket to process\n",
    "    :param tag_key: The key of the tag to look for (e.g., 'Project')\n",
    "    :param old_value: The current value of the tag that should be replaced\n",
    "    :param new_value: The new value to assign to the tag if matched\n",
    "    \"\"\"\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name)\n",
    "\n",
    "    for page in page_iterator:\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            try:\n",
    "                tag_response = s3_client.get_object_tagging(Bucket=bucket_name, Key=key)\n",
    "                tags = tag_response['TagSet']\n",
    "\n",
    "                tag_updated = False\n",
    "                for tag in tags:\n",
    "                    if tag['Key'] == tag_key and tag['Value'] == old_value:\n",
    "                        tag['Value'] = new_value\n",
    "                        tag_updated = True\n",
    "                        break\n",
    "\n",
    "                if tag_updated:\n",
    "                    s3_client.put_object_tagging(\n",
    "                        Bucket=bucket_name,\n",
    "                        Key=key,\n",
    "                        Tagging={'TagSet': tags}\n",
    "                    )\n",
    "                    print(f\"Updated tag '{tag_key}' from '{old_value}' to '{new_value}' for object: {key}\")\n",
    "            except s3_client.exceptions.NoSuchKey:\n",
    "                print(f\"Object not found: {key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing object {key}: {e}\")\n",
    "\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=s3_endpoint_url,\n",
    "    aws_access_key_id=S3_ACCESS_KEY,\n",
    "    aws_secret_access_key=S3_SECRET_KEY,\n",
    "    config=Config(\n",
    "        signature_version='s3v4',\n",
    "        parameter_validation=False,\n",
    "        s3={\n",
    "            'payload_signing_enabled': False, 'addressing_style': 'path', 'checksum_algorithm': None\n",
    "        }\n",
    "    ),\n",
    "    verify=False\n",
    ")\n",
    "\n",
    "#\n",
    "# Find all of the S3 Objects in the bucket that have a Project of \"GoM\" and update it to \"GoA\"\n",
    "#\n",
    "update_s3_object_tag_value(\n",
    "    s3_client=s3_client,\n",
    "    bucket_name=S3_bucket_name,\n",
    "    tag_key='Project',\n",
    "    old_value='GoM',\n",
    "    new_value='GoA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3ac8c-66a0-49c7-9e49-bd06d8853e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0944e296-9c8e-44d0-ae9d-3de2264c884d",
   "metadata": {},
   "source": [
    "# VAST Database Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db527d0-8589-43ac-a44a-175d5150d87b",
   "metadata": {},
   "source": [
    "## Install and Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32093f-0c6c-4d29-99ab-39d1527363c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11c54c-db85-4a30-bd1d-7114a54caec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36325d0-3aae-4ce3-8b12-21ea566b6cc6",
   "metadata": {},
   "source": [
    "## Load Demo State and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788158e-20dc-4195-ab22-e509b8759e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Demo Variables and values.\n",
    "with open(\"demo_state.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "# Dynamically create python variables needed for the Demo.\n",
    "for key, value in data.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "VASTDB_JARS_DIR = '/opt/vastdb_spark/'\n",
    "# Get a comma-separated string of all JARs in the directory\n",
    "jars = \",\".join([os.path.join(VASTDB_JARS_DIR, f) for f in os.listdir(VASTDB_JARS_DIR) if f.endswith(\".jar\")])\n",
    "jars_path = \":\".join([os.path.join(VASTDB_JARS_DIR, f) for f in os.listdir(VASTDB_JARS_DIR) if f.endswith(\".jar\")])\n",
    "DB_ENDPOINT = f\"http://{vip_pool_ip}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e75f1f-9d2a-4806-90ec-7f48bb0f0179",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17dcbf7-e950-4d1d-b256-e5e48ef3c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlcmd(sqlString):\n",
    "    try:\n",
    "        df_result = spark.sql(sqlString)\n",
    "    except AnalysisException as e:\n",
    "        print(f\"AnalysisException occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return df_result    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad362d-d3c4-477a-b0cf-1a391a5ff238",
   "metadata": {},
   "source": [
    "## Define Spark Instance Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f18a4b-2b56-4535-a337-ce5abc6a8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "     pyspark.SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", jars_path)\n",
    "        .set(\"spark.driver.userClassPathFirst\", \"true\")\n",
    "        .set(\"spark.executor.cores\",\"1\")\n",
    "        .set(\"spark.executor.userClassPathFirst\", \"true\")\n",
    "        .set(\"spark.jars\", jars)\n",
    "    # VAST Natural Database Configuration Options\n",
    "        .set(\"spark.ndb.access_key_id\",S3_ACCESS_KEY)\n",
    "        .set(\"spark.ndb.secret_access_key\",S3_SECRET_KEY)\n",
    "        .set(\"spark.ndb.endpoint\",DB_ENDPOINT)\n",
    "        .set(\"spark.ndb.dynamic_filter_compaction_threshold\", 100)\n",
    "        .set(\"spark.ndb.dynamic_filtering_wait_timeout\", 2)\n",
    "        .set(\"spark.ndb.parallel_import\", \"true\")\n",
    "        .set(\"spark.ndb.retry_max_count\",3)\n",
    "        .set(\"spark.ndb.retry_sleep_duration\",1)\n",
    "        .set(\"spark.sql.catalog.ndb\", \"spark.sql.catalog.ndb.VastCatalog\")\n",
    "        .set(\"spark.sql.extensions\",\"ndb.NDBSparkSessionExtension\")\n",
    "        .set(\"spark.ndb.num_of_splits\",8)\n",
    "        .set(\"spark.ndb.num_of_sub_splits\",8)\n",
    "        .set(\"spark.ndb.rowgroups_per_subsplit\",1)\n",
    "        .set(\"spark.ndb.query_data_rows_per_split\",4000000)\n",
    "        .set(\"spark.network.timeout\", \"3600s\")\n",
    "        .set(\"spark.port.maxRetries\", 30)\n",
    "        .set(\"spark.rpc.askTimeout\", \"3000s\")\n",
    "        .set(\"spark.rpc.io.connectionTimeout\", \"3000s\")\n",
    "   #    .set(\"spark.rpc.numRetries\", 15)\n",
    "   #    .set(\"spark.rpc.retry.wait\", \"3000s\") \n",
    "        .set(\"spark.shuffle.io.connectionTimeout\", \"3000s\")\n",
    "        .set(\"spark.shuffle.push.finalize.timeout\", \"3000s\")\n",
    "        .set(\"spark.speculation\", \"false\")\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .set(\"spark.sql.readSideCharPadding\",\"False\")\n",
    "        .set(\"spark.task.maxFailures\", \"1\")\n",
    "        .set(\"spark.task.reaper.pollingInterval\", \"3000s\")  \n",
    "        .setAppName('VASTDB')\n",
    "        .setMaster(\"local[2]\")\n",
    ")\n",
    "if SparkSession.getActiveSession() is not None:\n",
    "    # Stop the SparkSession\n",
    "    SparkSession.getActiveSession().stop()\n",
    "    print(\"SparkSession stopped.\")\n",
    "else:\n",
    "    print(\"No active SparkSession found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba4873d-fa50-4b85-94c3-ce12a95b5dd1",
   "metadata": {},
   "source": [
    "## Start Spark Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677a230-f567-4bb8-8144-dc821fd1754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Get the SparkConf from the SparkContext\n",
    "conf = spark.sparkContext.getConf()\n",
    "print(\"Spark Running\")\n",
    "spark_version = spark.sparkContext.version\n",
    "print(f\"Spark version: {spark_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba019c0a-c787-4004-87f1-fcfa00f43da5",
   "metadata": {},
   "source": [
    "## Create the \"Database / Schema\" for the Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b173d-b904-43a0-a7e6-01ded916a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_text = f\"CREATE DATABASE IF NOT EXISTS `ndb`.`{vastdb_bucket}`.`{demo_suffix}`\"\n",
    "sqlcmd(sql_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0da831-6d1e-4a0b-bfc5-cba39f600301",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = f\"USE `ndb`.`{vastdb_bucket}`.`{demo_suffix}`;\"\n",
    "sqlcmd(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9f64d-8511-4a67-ab4a-7c7c65deaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create the Table to store the events for IDrive.\n",
    "# \n",
    "sqlCMD = \"\"\"CREATE TABLE IF NOT EXISTS idrive (\n",
    "    eventSource STRING,\n",
    "    awsRegion STRING,\n",
    "    lastModified TIMESTAMP,\n",
    "    eventName STRING,\n",
    "    userIdentity_principalId STRING,\n",
    "    requestParameters_sourceIPAddress STRING,\n",
    "    responseElements_x_amz_request_id STRING,\n",
    "    responseElements_x_amz_id_2 STRING,\n",
    "    s3_configurationId STRING,\n",
    "    s3_bucket_name STRING,\n",
    "    s3_bucket_ownerIdentity_principalId STRING,\n",
    "    s3_bucket_arn STRING,\n",
    "    s3_object_key STRING,\n",
    "    s3_object_ext STRING,\n",
    "    s3_object_size BIGINT,\n",
    "    s3_object_sequencer STRING,\n",
    "    s3_tags MAP<STRING, STRING>);\"\"\"\n",
    "sqlcmd(sqlCMD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78aa52d-1432-4ec0-bb22-ae290899cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create the Table to store the S3 tags for IDrive.\n",
    "# \n",
    "sqlCMD = \"\"\"CREATE TABLE IF NOT EXISTS tag_metadata (\n",
    "    awsRegion STRING,\n",
    "    s3_bucket_name STRING,\n",
    "    s3_object_key STRING,\n",
    "    key STRING,\n",
    "    string_value STRING,\n",
    "    int_value INT,\n",
    "    float_value FLOAT, \n",
    "    bool_value BOOLEAN\n",
    "  \n",
    ");\"\"\"\n",
    "\n",
    "sqlcmd(sqlCMD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f5252-caed-40a4-88c9-3435c7fa6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# List the tables that are now in the \"database\"\n",
    "#\n",
    "sqlCMD = \"SHOW TABLES;\"\n",
    "df_result = sqlcmd(sqlCMD)\n",
    "\n",
    "# Data is returned in a Spark Dataframe\n",
    "df_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e112da3-325b-4be0-93e0-3150c5c31775",
   "metadata": {},
   "source": [
    "## Create SQL for TABLE Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e1c3c-dd51-4605-a3b1-38a419c1372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlCMD = \"SHOW CREATE TABLE idrive;\"\n",
    "df=sqlcmd(sqlCMD)\n",
    "pdf = df.select(\"createtab_stmt\").toPandas()\n",
    "pdf['createtab_stmt'] = pdf['createtab_stmt'].str.replace('\\n', '<br>')\n",
    "display(HTML(pdf.to_html(escape=False, index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5796add-6f27-497f-8dbc-8643c6a81776",
   "metadata": {},
   "source": [
    "## Query Object Metadata Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cac56-9507-44e5-9a9a-4921ca3ca710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from tag_metadata LIMIT 20;\")\n",
    "pdf = df.toPandas()\n",
    "pdf['int_value'] = pdf['int_value'].fillna(0).astype(int) # The toPandas() method doesnt capture the datatype correctly.\n",
    "display(HTML(pdf.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e8a54-a978-4645-ac57-78f54952045a",
   "metadata": {},
   "source": [
    "## Query IDrive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97024d67-4e96-4191-bae7-476430353319",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqltxt = \"\"\"\n",
    "SELECT\n",
    "    eventSource,\n",
    "    awsRegion,\n",
    "    lastModified, \n",
    "    eventName,\n",
    "    userIdentity_principalId,\n",
    "    requestParameters_sourceIPAddress,\n",
    "    responseElements_x_amz_request_id,\n",
    "    responseElements_x_amz_id_2,\n",
    "    s3_configurationId,\n",
    "    s3_bucket_name,\n",
    "    s3_bucket_ownerIdentity_principalId,\n",
    "    s3_bucket_arn,\n",
    "    s3_object_key,\n",
    "    s3_object_ext,\n",
    "    s3_object_size,\n",
    "    s3_object_sequencer\n",
    "FROM idrive LIMIT 20;\n",
    "\"\"\"  # LIMIT 20 , WHERE s3_object_key = 'site1/g34zduhh.xlsx\n",
    "df = spark.sql(sqltxt) \n",
    "if isinstance(df, pyspark.sql.dataframe.DataFrame) and not df.isEmpty():\n",
    "    pdf = df.toPandas()\n",
    "  #  pdf['int_value'] = pdf['int_value'].fillna(0).astype(int) # The toPandas() method doesnt capture the datatype correctly.\n",
    "    display(HTML(pdf.to_html(index=False)))\n",
    "else:\n",
    "    print(\"No records returned from SQL Query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fd485-faf1-4cba-b148-feebc994cf94",
   "metadata": {},
   "source": [
    "> Switch to the S3 Events - Object Source Notebook to create S3 Object Events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62162d-cb03-43c3-ab1e-0ed3617b2370",
   "metadata": {},
   "source": [
    "## SQL Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23ba58-6b9e-4cb8-86f8-b6204565f40e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Query how many EXCEL Files are in the idrive table.\n",
    "#\n",
    "print(\"EXCEL Files loaded:\")\n",
    "sqltxt = \"\"\"\n",
    " SELECT COUNT(*) AS TOTAL \n",
    "  FROM idrive \n",
    "  WHERE s3_object_ext = 'xlsx';\n",
    "\"\"\"\n",
    "df = spark.sql(sqltxt)\n",
    "pdf = df.toPandas()\n",
    "display(HTML(pdf.to_html(index=False)))\n",
    "\n",
    "#\n",
    "# Query the total number of Files that are in the idrive table.\n",
    "#\n",
    "print(\"TOTAL Files loaded:\")\n",
    "sqltxt = \"\"\"\n",
    " SELECT COUNT(*) AS TOTAL \n",
    "  FROM idrive;\n",
    "\"\"\"\n",
    "df = spark.sql(sqltxt)\n",
    "pdf = df.toPandas()\n",
    "display(HTML(pdf.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba670842-b204-471f-9cd9-d664bfa845ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Report on the number of files for each unique file extension. \n",
    "#\n",
    "sqltxt = \"\"\"\n",
    "SELECT \n",
    "    s3_object_ext, \n",
    "    COUNT(*) AS count, \n",
    "    SUM(s3_object_size) AS total_size\n",
    "FROM idrive\n",
    "GROUP BY s3_object_ext\n",
    "ORDER BY count DESC;\n",
    "\"\"\"\n",
    "df = spark.sql(sqltxt)\n",
    "pdf = df.toPandas()\n",
    "display(HTML(pdf.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2499db1-f6f8-4b4c-acb4-8fa25c723978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Report the Unique S3 user tags and how many files have each tag.\n",
    "#\n",
    "sqltxt = \"\"\"\n",
    "SELECT \n",
    "    key,\n",
    "    COUNT(*) AS record_count\n",
    "FROM \n",
    "    tag_metadata\n",
    "GROUP BY \n",
    "    key\n",
    "ORDER BY \n",
    "    record_count DESC;\n",
    "\"\"\"\n",
    "df = spark.sql(sqltxt)\n",
    "if isinstance(df, pyspark.sql.dataframe.DataFrame) and not df.isEmpty():\n",
    "    pdf = df.toPandas()\n",
    "    display(HTML(pdf.to_html(index=False)))\n",
    "else:\n",
    "    print(\"No records returned from SQL Query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7940a-5edd-457a-aac5-a5d95908ea4b",
   "metadata": {},
   "source": [
    "## Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71582f9a-2fcc-42aa-8881-23759502f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Query the S3 User tag metadata for each idrive (File) record that has a \"docx\" extension.\n",
    "#\n",
    "\n",
    "sqltxt= \"\"\"\n",
    "  SELECT\n",
    "    idrive.lastModified, \n",
    "    idrive.eventName,\n",
    "    idrive.s3_bucket_name,\n",
    "    idrive.s3_object_key,\n",
    "    idrive.s3_object_ext,\n",
    "    idrive.s3_object_size,\n",
    "    tag_metadata.key,\n",
    "    tag_metadata.string_value,\n",
    "    tag_metadata.int_value,\n",
    "    tag_metadata.float_value,\n",
    "    tag_metadata.bool_value\n",
    "FROM idrive\n",
    "JOIN tag_metadata\n",
    "ON idrive.awsRegion = tag_metadata.awsRegion\n",
    "   AND idrive.s3_bucket_name = tag_metadata.s3_bucket_name\n",
    "   AND idrive.s3_object_key = tag_metadata.s3_object_key\n",
    "WHERE idrive.s3_object_ext = 'docx' \n",
    "ORDER BY idrive.s3_object_key\n",
    "LIMIT 40; \n",
    "\"\"\"\n",
    "df = spark.sql(sqltxt)\n",
    "if isinstance(df, pyspark.sql.dataframe.DataFrame) and not df.isEmpty():\n",
    "    pdf = df.toPandas()\n",
    "    pdf['int_value'] = pdf['int_value'].fillna(0).astype(int) # The toPandas() method doesnt capture the datatype correctly.\n",
    "    display(HTML(pdf.to_html(index=False)))\n",
    "else:\n",
    "    print(\"No records returned from SQL Query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaee090-2445-4e7f-a74f-55a28f8ceb6d",
   "metadata": {},
   "source": [
    "#### Query for S3 User Tag : \"ASCII\" set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89358f17-760e-437a-81ff-61524593ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=[]\n",
    "sqltxt= \"\"\"\n",
    "SELECT\n",
    "    idrive.lastModified,  \n",
    "    idrive.eventName,\n",
    "    idrive.s3_bucket_name,\n",
    "    idrive.s3_object_key,\n",
    "    idrive.s3_object_ext,\n",
    "    idrive.s3_object_size,\n",
    "    tag_metadata.key,\n",
    "    tag_metadata.string_value,\n",
    "    tag_metadata.int_value,\n",
    "    tag_metadata.float_value,\n",
    "    tag_metadata.bool_value\n",
    "FROM idrive\n",
    "JOIN tag_metadata\n",
    "  ON idrive.awsRegion = tag_metadata.awsRegion\n",
    " AND idrive.s3_bucket_name = tag_metadata.s3_bucket_name\n",
    " AND idrive.s3_object_key = tag_metadata.s3_object_key\n",
    "WHERE tag_metadata.key = 'ASCII'\n",
    "  AND tag_metadata.bool_value IS FALSE\n",
    "LIMIT 40;\n",
    "\"\"\"\n",
    "df = spark.sql(sqltxt)\n",
    "if isinstance(df, pyspark.sql.dataframe.DataFrame) and not df.isEmpty():\n",
    "    pdf = df.toPandas()\n",
    "    pdf['int_value'] = pdf['int_value'].fillna(0).astype(int) # The toPandas() method doesnt capture the datatype correctly.\n",
    "    display(HTML(pdf.to_html(index=False)))\n",
    "    print(pdf.shape)\n",
    "else:\n",
    "    print(\"No records returned from SQL Query.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0799d18c-677f-4d60-8199-789bbbdb87a6",
   "metadata": {},
   "source": [
    "## SQL Table Aliasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea2eed-9584-4ad7-bfd7-6bb6002b92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Query for S3 objects associated with the \"EFS\"  Project, and have a PSI value between 2500 AND 3500\n",
    "#\n",
    "sqltxt = \"\"\"\n",
    "SELECT \n",
    "    idrive.lastModified,  \n",
    "    idrive.eventName,\n",
    "    idrive.s3_bucket_name,\n",
    "    idrive.s3_object_key,\n",
    "    idrive.s3_object_ext,\n",
    "    idrive.s3_object_size,\n",
    "    tm_project.key AS project_key,\n",
    "    tm_project.string_value AS project_value,\n",
    "    tm_psi.key AS psi_key,\n",
    "    tm_psi.int_value AS psi_value\n",
    "FROM idrive\n",
    "JOIN tag_metadata tm_project\n",
    "  ON idrive.awsRegion = tm_project.awsRegion\n",
    " AND idrive.s3_bucket_name = tm_project.s3_bucket_name\n",
    " AND idrive.s3_object_key = tm_project.s3_object_key\n",
    "JOIN tag_metadata tm_psi\n",
    "  ON idrive.awsRegion = tm_psi.awsRegion\n",
    " AND idrive.s3_bucket_name = tm_psi.s3_bucket_name\n",
    " AND idrive.s3_object_key = tm_psi.s3_object_key\n",
    "WHERE tm_project.key = 'Project'\n",
    "  AND tm_project.string_value = 'EFS'\n",
    "  AND tm_psi.key = 'PSI'\n",
    "  AND tm_psi.int_value BETWEEN 2500 AND 3500\n",
    "  ORDER BY idrive.s3_object_ext;\n",
    "\"\"\"\n",
    "df = spark.sql(sqltxt)\n",
    "if isinstance(df, pyspark.sql.dataframe.DataFrame) and not df.isEmpty():\n",
    "    pdf = df.toPandas()\n",
    "    pdf['psi_value'] = pdf['psi_value'].fillna(0).astype(int) # The toPandas() method doesnt capture the datatype correctly.\n",
    "    display(HTML(pdf.to_html(index=False)))\n",
    "    print(pdf.shape)\n",
    "else:\n",
    "    print(\"No records returned from SQL Query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d4fe1-792c-486d-9893-1a0941eabd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqltxt = \"\"\"\n",
    "SELECT \n",
    "    idrive.lastModified,  \n",
    "    idrive.eventName,\n",
    "    idrive.s3_bucket_name,\n",
    "    idrive.s3_object_key,\n",
    "    idrive.s3_object_ext,\n",
    "    idrive.s3_object_size,\n",
    "    tm_project.key AS project_key,\n",
    "    tm_project.string_value AS project_value,\n",
    "    tm_psi.key AS psi_key,\n",
    "    tm_psi.int_value AS psi_value\n",
    "FROM idrive\n",
    "JOIN tag_metadata tm_project\n",
    "  ON idrive.awsRegion = tm_project.awsRegion\n",
    " AND idrive.s3_bucket_name = tm_project.s3_bucket_name\n",
    " AND idrive.s3_object_key = tm_project.s3_object_key\n",
    "JOIN tag_metadata tm_psi\n",
    "  ON idrive.awsRegion = tm_psi.awsRegion\n",
    " AND idrive.s3_bucket_name = tm_psi.s3_bucket_name\n",
    " AND idrive.s3_object_key = tm_psi.s3_object_key\n",
    "WHERE tm_project.key = 'Project'\n",
    "  AND tm_project.string_value = 'GoA'\n",
    "  AND tm_psi.key = 'PSI'\n",
    "  AND tm_psi.int_value BETWEEN 2500 AND 3500\n",
    "ORDER BY idrive.s3_object_ext;\n",
    "\"\"\"\n",
    "df = spark.sql(sqltxt)\n",
    "if isinstance(df, pyspark.sql.dataframe.DataFrame) and not df.isEmpty():\n",
    "    pdf = df.toPandas()\n",
    "    pdf['psi_value'] = pdf['psi_value'].fillna(0).astype(int) # The toPandas() method doesnt capture the datatype correctly.\n",
    "    display(HTML(pdf.to_html(index=False)))\n",
    "    print(pdf.shape)\n",
    "else:\n",
    "    print(\"No records returned from SQL Query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dadd353-e574-45db-8f07-43db021dba49",
   "metadata": {},
   "source": [
    "## SQL Table management  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81aec8-6a18-4c22-b9a5-27e2c7273f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"DELETE FROM tag_metadata;\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb83ad-1727-45ef-8d86-98a9f8576d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"DELETE FROM idrive;\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ac7d8-17d7-4173-b6a4-eb93ef10be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqltxt = \"\"\"\n",
    "DROP TABLE idrive;\n",
    "DROP TABLE tag_metadata;\n",
    "\"\"\"\n",
    "df = spark.sql(sqltxt)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8af239-4fd0-4470-a281-1917e37eeebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

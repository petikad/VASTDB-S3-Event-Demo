{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d505cd9a-3eba-4dc1-96a1-d9c0cef0c341",
   "metadata": {},
   "source": [
    "# Kafka Demo Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465290b-f667-42d3-9843-99a153352780",
   "metadata": {},
   "source": [
    "## Install and Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2b001-6a78-43d9-9397-701cf9b55d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3dbd63-2cbf-482f-8c0f-c03805c9772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vastdb\n",
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer , TopicPartition\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import urllib.parse\n",
    "from collections import defaultdict\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f8c66-8e39-4bc2-92af-628092c863ad",
   "metadata": {},
   "source": [
    "## SDK Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e8ab0-25b3-460f-a8ec-e5172eedc2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,    \n",
    "    format='%(asctime)s - %(levelname)s - %(funcName)s - %(message)s', \n",
    "    handlers=[\n",
    "        logging.FileHandler('vastdb_sdk.log', mode = 'a'), \n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f48e1-5317-467f-a2e3-935b98e9e5b1",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a701a-6917-4cf6-9a9b-4f84a4153644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tag_value_type(tag_map, object_id):\n",
    "    \"\"\" \n",
    "    Infers the data type of each tag value from an S3 object's tag map and \n",
    "    returns a dictionary of column-aligned lists for insertion into a structured table.\n",
    "\n",
    "    Parameters:\n",
    "        tag_map (dict): A dictionary of S3 user-defined tags in the form {key: value}, \n",
    "                        where the value is a string.\n",
    "        object_id (tuple): A tuple in the format (awsRegion, bucket_name, object_key) \n",
    "                           that identifies the S3 object associated with the tags.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with column names as keys and lists of values as values.\n",
    "              Each list corresponds to a column in a table (e.g., tag_metadata), and\n",
    "              contains values inferred as one of the supported types: string, int, \n",
    "              float, or boolean. Only one type column will be populated per row.\n",
    "    \"\"\"\n",
    "    awsRegion_list = []\n",
    "    bucket_list = []\n",
    "    object_key_list = []\n",
    "    key_list = []\n",
    "    string_value_list = []\n",
    "    int_value_list = []\n",
    "    float_value_list = []\n",
    "    bool_value_list = []\n",
    "\n",
    "    for k, v in tag_map.items():\n",
    "        awsRegion_list.append(object_id[0])\n",
    "        bucket_list.append(object_id[1])\n",
    "        object_key_list.append(object_id[2])\n",
    "        key_list.append(k)\n",
    "\n",
    "        val = v.strip() if isinstance(v, str) else str(v).strip()\n",
    "\n",
    "        # Try boolean\n",
    "        if val.lower() in ['true', 'false']:\n",
    "            string_value_list.append(None)\n",
    "            int_value_list.append(None)\n",
    "            float_value_list.append(None)\n",
    "            bool_value_list.append(val.lower() == 'true')\n",
    "\n",
    "        # Try int\n",
    "        elif val.isdigit() or (val.startswith('-') and val[1:].isdigit()):\n",
    "            try:\n",
    "                int_val = int(val)\n",
    "                string_value_list.append(None)\n",
    "                int_value_list.append(int_val)\n",
    "                float_value_list.append(None)\n",
    "                bool_value_list.append(None)\n",
    "            except ValueError:\n",
    "                string_value_list.append(val)\n",
    "                int_value_list.append(None)\n",
    "                float_value_list.append(None)\n",
    "                bool_value_list.append(None)\n",
    "\n",
    "        # Try float\n",
    "        else:\n",
    "            try:\n",
    "                float_val = float(val)\n",
    "                string_value_list.append(None)\n",
    "                int_value_list.append(None)\n",
    "                float_value_list.append(float_val)\n",
    "                bool_value_list.append(None)\n",
    "            except ValueError:\n",
    "                string_value_list.append(val)\n",
    "                int_value_list.append(None)\n",
    "                float_value_list.append(None)\n",
    "                bool_value_list.append(None)\n",
    "\n",
    "    return {\n",
    "        'awsRegion': awsRegion_list,\n",
    "        's3_bucket_name': bucket_list,\n",
    "        's3_object_key': object_key_list,\n",
    "        'key': key_list,\n",
    "        'string_value': string_value_list,\n",
    "        'int_value': int_value_list,\n",
    "        'float_value': float_value_list,\n",
    "        'bool_value': bool_value_list\n",
    "    }\n",
    "\n",
    "def get_s3_object_metadata(object_id, s3_client):\n",
    "    \"\"\"\n",
    "    Retrieves metadata for a specific S3 object.\n",
    "\n",
    "    Parameters:\n",
    "        object_id (tuple): A 3-element tuple in the format \n",
    "                           (awsRegion, bucket_name, object_key),\n",
    "                           identifying the S3 object.\n",
    "        s3_client (boto3.client): An initialized boto3 S3 client.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing standard S3 object metadata fields:\n",
    "              - 'Size': The object's size in bytes.\n",
    "              - 'LastModified': The datetime the object was last modified.\n",
    "              - 'ETag': The entity tag (usually an MD5 hash).\n",
    "              - 'ContentType': The object's MIME type.\n",
    "              - 'Metadata': Any user-defined metadata (as a sub-dict).\n",
    "              \n",
    "              Returns an empty dictionary if the object doesn't exist\n",
    "              or if an error occurs during the request.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        response = s3_client.head_object(\n",
    "                Bucket=object_id[1],\n",
    "                Key=object_id[2])\n",
    "        return {\n",
    "            'Size': response.get('ContentLength'),\n",
    "            'LastModified': response.get('LastModified'),\n",
    "            'ETag': response.get('ETag'),\n",
    "            'ContentType': response.get('ContentType'),\n",
    "            'Metadata': response.get('Metadata', {})  # Custom metadata if any\n",
    "        }\n",
    "        \n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(f\"Object '{object_id[2]}' not found in bucket '{object_id[1]}'.\")\n",
    "    except s3_client.exceptions.ClientError as e:\n",
    "        print(f\"Client error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return {}  # Return empty dict on failure\n",
    "    \n",
    "def build_idrive_recordbatch(\n",
    "    object_id: tuple,\n",
    "    s3_client,\n",
    "    s3_tags: dict,\n",
    "    s3_object_ext: str\n",
    ") -> pa.RecordBatch:\n",
    "    \"\"\"\n",
    "    Builds an iDRive PyArrow RecordBatch for a single S3 object using metadata and tags.\n",
    "    \n",
    "    Parameters:\n",
    "        object_id (tuple): A 3-element tuple in the format (awsRegion, bucket_name, object_key)\n",
    "                           identifying the S3 object.\n",
    "        s3_client (boto3.client): An initialized Boto3 S3 client used to query the object's metadata.\n",
    "        s3_tags (dict): A dictionary of S3 user-defined tags associated with the object.\n",
    "        s3_object_ext (str): The file extension of the S3 object (e.g., 'txt', 'csv').\n",
    "    \n",
    "    Returns:\n",
    "        pyarrow.RecordBatch: A record batch containing all fields required by the 'idrive' table schema.\n",
    "                             It includes metadata fields like lastModified, object size, and user tags.\n",
    "                             Missing or optional event-related values are initialized as None.\n",
    "    \"\"\"\n",
    "\n",
    "    obj_metadata = get_s3_object_metadata(object_id, s3_client)\n",
    " \n",
    "    # Default values for missing fields in the schema\n",
    "    eventSource = None\n",
    "    s3_bucket_name = object_id[1]\n",
    "    s3_object_key = object_id[2]\n",
    "    awsRegion = object_id[0]\n",
    "    lastModified = obj_metadata['LastModified']\n",
    "    eventName = None\n",
    "    userIdentity_principalId = None\n",
    "    requestParameters_sourceIPAddress = None\n",
    "    responseElements_x_amz_request_id = None\n",
    "    responseElements_x_amz_id_2 = None\n",
    "    s3_configurationId = None\n",
    "    s3_bucket_ownerIdentity_principalId = None\n",
    "    s3_bucket_arn = None\n",
    "    s3_object_size = obj_metadata['Size']\n",
    "    s3_object_sequencer = None\n",
    "\n",
    "    # Define schema\n",
    "    schema = pa.schema([\n",
    "        ('eventSource', pa.string()),\n",
    "        ('awsRegion', pa.string()),\n",
    "        ('lastModified', pa.timestamp('ms')),\n",
    "        ('eventName', pa.string()),\n",
    "        ('userIdentity_principalId', pa.string()),\n",
    "        ('requestParameters_sourceIPAddress', pa.string()),\n",
    "        ('responseElements_x_amz_request_id', pa.string()),\n",
    "        ('responseElements_x_amz_id_2', pa.string()),\n",
    "        ('s3_configurationId', pa.string()),\n",
    "        ('s3_bucket_name', pa.string()),\n",
    "        ('s3_bucket_ownerIdentity_principalId', pa.string()),\n",
    "        ('s3_bucket_arn', pa.string()),\n",
    "        ('s3_object_ext', pa.string()),\n",
    "        ('s3_object_key', pa.string()),\n",
    "        ('s3_object_size', pa.int64()),\n",
    "        ('s3_object_sequencer', pa.string()),\n",
    "        ('s3_tags', pa.map_(pa.string(), pa.string()))\n",
    "    ])\n",
    "\n",
    "    # Build record as a list of columns (each a list of one element)\n",
    "    record = [\n",
    "        [eventSource],\n",
    "        [awsRegion],\n",
    "        [lastModified],\n",
    "        [eventName],\n",
    "        [userIdentity_principalId],\n",
    "        [requestParameters_sourceIPAddress],\n",
    "        [responseElements_x_amz_request_id],\n",
    "        [responseElements_x_amz_id_2],\n",
    "        [s3_configurationId],\n",
    "        [s3_bucket_name],\n",
    "        [s3_bucket_ownerIdentity_principalId],\n",
    "        [s3_bucket_arn],\n",
    "        [s3_object_ext],\n",
    "        [s3_object_key],\n",
    "        [s3_object_size],\n",
    "        [s3_object_sequencer],\n",
    "        [s3_tags]\n",
    "    ]\n",
    "\n",
    "    # Create and return RecordBatch\n",
    "    return pa.record_batch(record, schema=schema)\n",
    "\n",
    "def get_s3_object_tags(object_id, s3_client, return_map: bool = False):\n",
    "    \"\"\"\n",
    "    Retrieves the tags associated with a specific S3 object and returns either a dictionary or a PyArrow RecordBatch,\n",
    "    depending on the `return_map` flag.\n",
    "    \n",
    "    Parameters:\n",
    "        object_id (tuple): A 3-element tuple representing the S3 object in the form (awsRegion, bucket_name, object_key).\n",
    "        s3_client (boto3.client): An initialized Boto3 S3 client used to access S3 APIs.\n",
    "        return_map (bool, optional): If True, returns a dictionary of tag key-value pairs. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        dict or pyarrow.RecordBatch:\n",
    "            - If return_map is True: returns a dictionary mapping tag keys to their string values.\n",
    "            - If return_map is False: returns a PyArrow RecordBatch with typed columns for each tag\n",
    "              (string, int, float, bool), along with identifying fields like region and object key.\n",
    "    \n",
    "    Functionality:\n",
    "        - Uses the S3 client's `get_object_tagging` to retrieve user-defined tags.\n",
    "        - Optionally returns raw tag data or performs type inference to organize the data into a structured RecordBatch.\n",
    "        - Handles S3-specific errors gracefully and returns an empty dict on failure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare empty column arrays\n",
    "    awsRegion_list = []\n",
    "    bucket_list = []\n",
    "    object_key_list = []\n",
    "    key_list = []\n",
    "    string_value_list = []\n",
    "    int_value_list = []\n",
    "    float_value_list = []\n",
    "    bool_value_list = []\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.get_object_tagging(\n",
    "            Bucket=object_id[1],\n",
    "            Key=object_id[2]\n",
    "        )\n",
    "\n",
    "        tag_set = response.get('TagSet', [])\n",
    "        tag_map = {tag['Key']: tag['Value'] for tag in tag_set}\n",
    "        if return_map:\n",
    "          return tag_map\n",
    "            \n",
    "        # Type inference and column population    \n",
    "        tag_columns = infer_tag_value_type(tag_map, object_id)    \n",
    "    \n",
    "        # Define PyArrow schema\n",
    "        schema = pa.schema([\n",
    "            ('awsRegion', pa.string()),\n",
    "            ('s3_bucket_name', pa.string()),\n",
    "            ('s3_object_key', pa.string()),\n",
    "            ('key', pa.string()),\n",
    "            ('string_value', pa.string()),\n",
    "            ('int_value', pa.int32()),\n",
    "            ('float_value', pa.float32()),\n",
    "            ('bool_value', pa.bool_())\n",
    "        ])\n",
    "        \n",
    "        # Create RecordBatch\n",
    "        return pa.record_batch(tag_columns, schema=schema)\n",
    "\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(f\"Object '{object_id[2]}' not found in bucket '{object_id[1]}'.\")\n",
    "    except s3_client.exceptions.ClientError as e:\n",
    "        print(f\"Client error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return {}  # Return empty dict on failure\n",
    "\n",
    "\n",
    "def create_db_record(object_id, s3_client, db_session):\n",
    "    \"\"\"\n",
    "    Creates and inserts metadata records for an S3 object into the iDrive VAST database.\n",
    "    \n",
    "    Parameters:\n",
    "        object_id (tuple): A 3-element tuple representing the S3 object in the form (awsRegion, bucket_name, object_key).\n",
    "        s3_client (boto3.client): An initialized Boto3 S3 client used to retrieve tags and metadata from the S3 object.\n",
    "        db_session: A database session used to access and modify tables within the VAST database.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Functionality:\n",
    "        - Retrieves typed S3 object tags as a PyArrow RecordBatch and raw key-value map.\n",
    "        - Extracts the object's file extension (if present) for metadata tracking.\n",
    "        - Builds a second RecordBatch containing standard S3 object metadata.\n",
    "        - Opens a database transaction and inserts both tag metadata and object metadata into the appropriate tables\n",
    "          (\"tag_metadata\" and \"idrive\") within a VAST DB schema.\n",
    "    \"\"\"\n",
    "    print(f\"\\rCREATE: {object_id}\")\n",
    "    pa_recordbatch_tags = None\n",
    "    pa_recordbatch_tags = get_s3_object_tags(object_id, s3_client)\n",
    "    if pa_recordbatch_tags: \n",
    "        s3_tags = get_s3_object_tags(object_id, s3_client, return_map = True)\n",
    "        \n",
    "        if '.' in object_id[2]:\n",
    "            s3_object_ext = object_id[2].rsplit('.', 1)[-1]\n",
    "        else:\n",
    "            s3_object_ext = None  \n",
    "            \n",
    "        pa_recordbatch_idrive = build_idrive_recordbatch(object_id, s3_client, s3_tags, s3_object_ext)\n",
    "        with db_session.transaction() as tx:\n",
    "            bucket = tx.bucket(vastdb_bucket)       # Start a transaction for this Database\n",
    "            schema = bucket.schema(demo_suffix)     # Get the Schema for the database to use\n",
    "            tagmeta = schema.table(\"tag_metadata\")  # Get the table from the Schema.\n",
    "            tagmeta.insert(pa_recordbatch_tags)     # Insert the data from the Arrow table into the VAST DB table.\n",
    "            idrive = schema.table(\"idrive\")\n",
    "            idrive.insert(pa_recordbatch_idrive)\n",
    "    else:\n",
    "        print(f\"Failed to create database entry for {object_id[1]}\\\\{object_id[2]}.\")\n",
    "        \n",
    "def update_db_record(object_id, s3_client, db_session):\n",
    "    \"\"\"\n",
    "         This function updates the metadata and tag records of an S3 object in the iDrive VAST database.\n",
    "    \n",
    "    Parameters:\n",
    "     - object_id: A tuple (awsRegion, bucket_name, object_key) that uniquely identifies an S3 object.\n",
    "     - s3_client: A Boto3 S3 client instance used to retrieve object metadata and tags.\n",
    "     - db_session: An active session to the VAST database used for reading, updating, and inserting records.\n",
    "    \n",
    "    Functionality:\n",
    "     - Logs the update action for the specified object.\n",
    "     - Retrieves the latest S3 tags and infers their types (string, int, float, bool).\n",
    "     - Retrieves the object’s file extension, if present.\n",
    "     - Builds two PyArrow RecordBatches:\n",
    "        - One for the idrive table, containing metadata about the object.\n",
    "        - One for the tag_metadata table, containing the parsed and typed tags.\n",
    "     - Begins a transaction with the VAST database.\n",
    "     - For the idrive table:\n",
    "        - Deletes the existing record matching the object_id.\n",
    "        - Inserts the new metadata record.\n",
    "     - For the tag_metadata table:\n",
    "        - Reads existing tag records for the object.\n",
    "        - For each tag:\n",
    "           - If it already exists and any of its values differ, it is updated.\n",
    "           - If it's a new tag, it is inserted.\n",
    "     - All database changes occur within the same transaction to maintain consistency.\n",
    "    \"\"\"\n",
    "    print(f\"\\rUPDATE: {object_id}\")\n",
    "    \n",
    "    pa_recordbatch_tags = get_s3_object_tags(object_id, s3_client)\n",
    "    # Get the user tags from the S3 Object\n",
    "    s3_tags = get_s3_object_tags(object_id, s3_client, return_map=True)\n",
    "    # Infer the data types of the tags for storage / comparison with tag_metadata.\n",
    "    tag_columns = infer_tag_value_type(s3_tags, object_id)\n",
    "    if '.' in object_id[2]:\n",
    "        s3_object_ext = object_id[2].rsplit('.', 1)[-1]\n",
    "    else:\n",
    "        s3_object_ext = None\n",
    "\n",
    "    pa_recordbatch_idrive = build_idrive_recordbatch(object_id, s3_client, s3_tags, s3_object_ext)\n",
    "\n",
    "    with db_session.transaction() as tx:\n",
    "        bucket = tx.bucket(vastdb_bucket)\n",
    "        schema = bucket.schema(demo_suffix)\n",
    "\n",
    "        idrive = schema.table(\"idrive\")\n",
    "        idrive_reader = idrive.select(columns=[], internal_row_id=True,\n",
    "            predicate=(idrive['awsRegion'] == object_id[0]) &\n",
    "                      (idrive['s3_bucket_name'] == object_id[1]) &\n",
    "                      (idrive['s3_object_key'] == object_id[2]))\n",
    "        idrive_results = idrive_reader.read_all()\n",
    "        if idrive_results.num_rows > 0:\n",
    "            idrive.delete(idrive_results)\n",
    "        # Really an Exception if there wasnt a record found...    \n",
    "        idrive.insert(pa_recordbatch_idrive)\n",
    "\n",
    "        tagmeta = schema.table(\"tag_metadata\")\n",
    "        reader = tagmeta.select(columns=['key', 'string_value', 'int_value', 'float_value', 'bool_value'],\n",
    "                                internal_row_id=True,\n",
    "                                predicate=(tagmeta['awsRegion'] == object_id[0]) &\n",
    "                                          (tagmeta['s3_bucket_name'] == object_id[1]) &\n",
    "                                          (tagmeta['s3_object_key'] == object_id[2]))\n",
    "        # Convert pyarrow record to a python dict.\n",
    "        existing = {r['key']: r for r in reader.read_all().to_pylist()}\n",
    "        update_cols = ['string_value', 'int_value', 'float_value', 'bool_value']\n",
    "\n",
    "        for k, v in s3_tags.items():\n",
    "            # From the inferred data types build a record so we can compare the old and new. \n",
    "            indices = [i for i, val in enumerate(tag_columns['key']) if val == k]\n",
    "            new_row = {k1: v1[indices[0]] for k1, v1 in tag_columns.items()}\n",
    "\n",
    "            if k in existing:\n",
    "                existing_row = existing[k]\n",
    "                # Any tag change results in all the tags being updated.\n",
    "                if any(new_row[col] != existing_row.get(col) for col in update_cols):\n",
    "                    print(f\"Updating tag key: {k}\")\n",
    "                    # Append $row_id to enable update\n",
    "                    updated = pa.RecordBatch.from_pylist([new_row]).append_column(\n",
    "                        '$row_id', pa.array([existing_row['$row_id']])\n",
    "                    )\n",
    "                    tagmeta.update(updated, update_cols)\n",
    "            else:\n",
    "                print(f\"Inserting new tag key: {k}\")\n",
    "                tagmeta.insert(pa.Table.from_pylist([new_row]))\n",
    "\n",
    "\n",
    "def delete_db_record(object_id, s3_client, db_session):\n",
    "    \"\"\"\n",
    "    Deletes metadata records associated with a specific S3 object from the iDrive VAST database.\n",
    "    \n",
    "    Parameters:\n",
    "        object_id (tuple): A tuple in the format (awsRegion, bucket_name, object_key) identifying the S3 object.\n",
    "        s3_client (boto3.client): An initialized Boto3 S3 client (not used directly in this function but kept for interface consistency).\n",
    "        db_session: A database session object used to interact with the VAST database.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Functionality:\n",
    "        - Begins a transaction on the VAST database.\n",
    "        - Accesses the \"tag_metadata\" and \"idrive\" tables from the schema defined by the global `demo_suffix`.\n",
    "        - Queries each table for rows matching the given object_id values (region, bucket, key).\n",
    "        - If matching rows are found, deletes them from the respective table and prints the number of deleted rows.\n",
    "    \"\"\"\n",
    "    print(f\"\\rDELETE: {object_id}\")\n",
    "    with db_session.transaction() as tx:\n",
    "        bucket = tx.bucket(vastdb_bucket)\n",
    "        schema = bucket.schema(demo_suffix)\n",
    "        tagmeta = schema.table(\"tag_metadata\")\n",
    "        reader = tagmeta.select(columns=[], \n",
    "                              internal_row_id=True,\n",
    "                              predicate=(tagmeta['awsRegion']== object_id[0]) &\n",
    "                                        (tagmeta['s3_bucket_name']== object_id[1]) &\n",
    "                                        (tagmeta['s3_object_key']== object_id[2]))\n",
    "        results = reader.read_all() # Are there any ROWIDs to delete?\n",
    "        if results.num_rows > 0 :\n",
    "            print(f\"Deleting {results.num_rows} rows from {tagmeta.name}.\")\n",
    "            tagmeta.delete(results)\n",
    "        else:\n",
    "            print(f\"No {tagmeta.name} records found for S3 Object.\")\n",
    "        idrive = schema.table(\"idrive\")\n",
    "        reader = idrive.select(columns=[], \n",
    "                              internal_row_id=True,\n",
    "                              predicate=(idrive['awsRegion']== object_id[0]) &\n",
    "                                        (idrive['s3_bucket_name']== object_id[1]) &\n",
    "                                        (idrive['s3_object_key']== object_id[2]))\n",
    "        results = reader.read_all() # Are there any ROWIDs to delete?\n",
    "        if results.num_rows > 0 :\n",
    "            print(f\"Deleting {results.num_rows} rows from {idrive.name}.\")\n",
    "            idrive.delete(results)\n",
    "        else:\n",
    "            print(f\"No {idrive.name} records found for S3 Object.\")    \n",
    "\n",
    "class ObjectStateManager:\n",
    "    \"\"\"\n",
    "    A class that tracks and manages the state of S3 objects based on a stream of event records.\n",
    "\n",
    "    The class processes a batch of S3 event data, groups events by object, and determines whether\n",
    "    an object should be created, updated, deleted, or ignored in the database based on the sequence of events.\n",
    "    \"\"\"\n",
    "    def __init__(self, s3_client,db_session):\n",
    "        if s3_client is None:\n",
    "            raise ValueError(\"s3 boto client must be provided\")\n",
    "        if db_session is None:\n",
    "            raise ValueError(\"Vast Database Session must be provided\")\n",
    "        self.s3_client = s3_client\n",
    "        self.db_session = db_session\n",
    "        self.events_by_object = defaultdict(list)\n",
    "\n",
    "    def process_stream(self, df):\n",
    "        \"\"\"\n",
    "          Process a DataFrame of event records to determine final object states.\n",
    "\n",
    "        :param df: Pandas DataFrame with S3 event records. Each row must contain eventTime, eventName,\n",
    "                   awsRegion, s3.bucket.name, and s3.object.key columns.\n",
    "        \"\"\"\n",
    "        # Sort by eventTime\n",
    "        df = df.sort_values('eventTime')\n",
    "        \n",
    "        # Group events by unique resource\n",
    "        for _, row in df.iterrows():\n",
    "            object_id = self._get_object_id(row)\n",
    "            self.events_by_object[object_id].append(row['eventName'])\n",
    "\n",
    "        # Resolve final state per resource\n",
    "        for object_id, events in self.events_by_object.items():\n",
    "            self._resolve_object_events(object_id, events)\n",
    "\n",
    "        # Clear for next batch\n",
    "        self.events_by_object.clear()\n",
    "\n",
    "    def _resolve_object_events(self, object_id, events):\n",
    "        \"\"\"\n",
    "          Determine the final state of an S3 object based on its list of events,\n",
    "          and apply the appropriate database operation.\n",
    "\n",
    "        :param object_id: A tuple (awsRegion, bucket_name, object_key) identifying the S3 object\n",
    "        :param events: A list of event names for the given object\n",
    "        \"\"\"\n",
    "        state = None\n",
    "        for event in events:\n",
    "            if event.startswith('ObjectCreated:'):\n",
    "                if state == \"Deleted\":\n",
    "                    # Deleted then re-created: treat as created\n",
    "                    state = \"Created\"\n",
    "                elif state is None:\n",
    "                    state = \"Created\"\n",
    "                # An Error could be raised if the state was Updated before created...    \n",
    "            elif event.startswith('ObjectTagging'):\n",
    "                if state == \"Created\":\n",
    "                    state = \"Created\"  # Creation subsumes update\n",
    "                elif state is None:\n",
    "                    state = \"Updated\"\n",
    "            elif event.startswith('ObjectRemoved:'):\n",
    "                if state == \"Created\":\n",
    "                    state = None  # Created + Deleted cancels out\n",
    "                elif state is None:\n",
    "                    state = \"Deleted\"\n",
    "\n",
    "        # Execute the correct function\n",
    "        if state == \"Created\":\n",
    "            create_db_record(object_id, self.s3_client, self.db_session)\n",
    "        elif state == \"Updated\":\n",
    "            update_db_record(object_id, self.s3_client, self.db_session)\n",
    "        elif state == \"Deleted\":\n",
    "            delete_db_record(object_id, self.s3_client, self.db_session)\n",
    "        else:    \n",
    "            print(f\"Ignoring object: {object_id}.\")\n",
    "\n",
    "    def _get_object_id(self, row):\n",
    "        \"\"\"\n",
    "          Generate a unique object identifier tuple from a row of event data.\n",
    "\n",
    "        :param row: A pandas Series (row) representing an event and object\n",
    "        :return: Tuple of (awsRegion, bucket_name, object_key)\n",
    "        \"\"\"        \n",
    "        return (row['awsRegion'], row['s3.bucket.name'], row['s3.object.key'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b90ce7-f22a-4255-9d2b-687d84fa34ac",
   "metadata": {},
   "source": [
    "## Load Demo State and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee49f1b-a631-4f9c-acba-001655de4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Demo Variables and values.\n",
    "with open(\"demo_state.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "# Dynamically create python variables needed for the Demo.\n",
    "for key, value in data.items():\n",
    "    globals()[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852be858-78b7-4717-86ca-d4d62ab0c8bc",
   "metadata": {},
   "source": [
    "## Create Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873f290-839b-4e4a-ab7e-cace31f02db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.CRITICAL)\n",
    "consumer = KafkaConsumer(\n",
    "    kafka_topic, \n",
    "    bootstrap_servers=[vip_pool_ip],\n",
    "    auto_offset_reset='earliest',         # Start from the beginning of the topic\n",
    "    enable_auto_commit=False,  # Disable to avoid background thread\n",
    "    consumer_timeout_ms=2000,  # Return if no message in 1 sec\n",
    "    group_id='consumer-group',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba48e6-5c04-496a-90da-a71b37aab306",
   "metadata": {},
   "source": [
    "## Listen to topic for events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dba746-fd96-4659-8b9b-22048808d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Listening to topic '{kafka_topic}'...\\n\")\n",
    "messages_data = []\n",
    "# Inactivity timeout in seconds\n",
    "inactivity_timeout = 5\n",
    "# Time-limited consumption\n",
    "start_time = time.time()\n",
    "last_message_time = time.time()\n",
    "\n",
    "while True:\n",
    "    message_pack = consumer.poll(timeout_ms=2000)\n",
    "    new_message_received = False\n",
    "\n",
    "    for tp, messages in message_pack.items():\n",
    "        for message in messages:\n",
    "            try:\n",
    "                # Decode and parse the JSON message\n",
    "                \n",
    "                msg_value = message.value\n",
    "                save_it = message\n",
    "                if 'Records' in msg_value: \n",
    "                  for record in message.value['Records']:\n",
    "                      enriched_record = {\n",
    "                            **record,\n",
    "                            'kafka_offset': message.offset,\n",
    "                            'kafka_key': message.key.decode('utf-8') if isinstance(message.key, bytes) else str(message.key)\n",
    "                        }\n",
    "                      messages_data.append(enriched_record) \n",
    "                  new_message_received = True\n",
    "                  \n",
    "            except (json.JSONDecodeError, AttributeError) as e:\n",
    "                print(f\"Failed to parse message: {message.value} - Error: {e}\")\n",
    "\n",
    "    if new_message_received:\n",
    "        last_message_time = time.time()\n",
    "\n",
    "    if time.time() - last_message_time > inactivity_timeout:\n",
    "        print(f\"\\nNo new messages for {inactivity_timeout} seconds. Stopping listener.\")\n",
    "        break\n",
    "consumer.commit()  \n",
    "consumer.close()\n",
    "print(\"\\nStopped listening.\")\n",
    "\n",
    "logging.disable(logging.NOTSET)\n",
    "# Convert to a pandas DataFrame\n",
    "df1 = pd.json_normalize(messages_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fc2d3-9c39-4266-9e5f-ad2976381ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df1.empty:\n",
    "    print('No new messages processed.')\n",
    "else:    \n",
    "    #\n",
    "    # Convert Object data type to datetime and remove url parse .\n",
    "    #\n",
    "    df1['eventTime'] = pd.to_datetime(df1['eventTime'])\n",
    "    df1['s3.object.key'] = df1['s3.object.key'].apply(urllib.parse.unquote)\n",
    "    if 'result' in locals():\n",
    "      result = pd.concat([result, df1], ignore_index=True)\n",
    "    else:\n",
    "      result = df1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc700b9-1210-4cfb-b679-78507b12dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a1b0b-42e8-4861-b7ba-ff151665bfe0",
   "metadata": {},
   "source": [
    "## Process Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a7366-ac28-44c9-a9fc-de25712ffd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = f\"http://{vip_pool_ip}\"\n",
    "#\n",
    "# Create an S3 Client used to query Objects\n",
    "#\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=endpoint_url,\n",
    "    aws_access_key_id=S3_ACCESS_KEY,\n",
    "    aws_secret_access_key=S3_SECRET_KEY,\n",
    "    config=Config(signature_version='s3v4', \n",
    "                  parameter_validation=False, \n",
    "                  s3={'payload_signing_enabled':False,'addressing_style':'path','checksum_algorithm': None}\n",
    "                 ),\n",
    "    verify=False  # Set to False if the endpoint doesn't use SSL (http)\n",
    ")\n",
    "\n",
    "#\n",
    "# Establish Session with VAST database\n",
    "#\n",
    "db_session = {}\n",
    "try:\n",
    "    db_session = vastdb.connect(\n",
    "              endpoint=endpoint_url,\n",
    "              access=S3_ACCESS_KEY,\n",
    "              secret=S3_SECRET_KEY\n",
    "             )\n",
    "except Exception as e:\n",
    "    log.critical(e)\n",
    "if db_session:\n",
    "   log.info(\"VAST DB Session started\")\n",
    "else:\n",
    "   log.critical(\"Unable to connect to VAST DB.\")\n",
    "\n",
    "#\n",
    "# Instantiate OSM Class\n",
    "#\n",
    "osm = ObjectStateManager(s3_client, db_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f20c5-70b1-48f8-8b46-0d07bfe632d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: Once the Events have been processed set the \"result\" variable to None.\n",
    "#       This will ensure that events are not processed more than once.\n",
    "#\n",
    "osm.process_stream(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724198e2-8267-4772-b59c-3664c217c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf7057-86e3-4fae-ba66-a21ff10c3a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
